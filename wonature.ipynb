import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import Descriptors
from rdkit.ML.Descriptors.MoleculeDescriptors import MolecularDescriptorCalculator
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import tempfile
import os

def is_valid_smiles(smiles):
    try:
        mol = Chem.MolFromSmiles(smiles)
        return mol is not None
    except:
        return False

def preprocess_and_train():
    try:
        # Step 1: Load the dataset
        file_path = 'chemprop_data_cleaned_with_descriptors.csv'
        print("Loading dataset...")
        df = pd.read_csv(file_path)
        
        # Step 2: Clean data
        print("Cleaning data...")
        df = df.drop_duplicates().dropna()
        
        # Step 3: Filter invalid SMILES
        print("Filtering invalid SMILES...")
        valid_smiles_mask = df['SMILES'].apply(is_valid_smiles)
        df = df[valid_smiles_mask]
        print(f"Valid SMILES remaining: {len(df)}")
        
        # Step 4: Convert to numeric
        numeric_columns = df.select_dtypes(include=[np.number]).columns
        df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')
        
        # Step 5: Handle infinities
        df.replace([np.inf, -np.inf], np.nan, inplace=True)
        df.dropna(inplace=True)
        
        # Step 6: Process target column
        df['pIC50'] = pd.to_numeric(df['pIC50'], errors='coerce')
        df.dropna(subset=['pIC50'], inplace=True)
        
        # Step 7: Prepare features
        print("Preparing features...")
        X = df.drop(columns=['SMILES', 'pIC50']).select_dtypes(include=[np.number])
        y = df['pIC50']
        
        # Step 8: Scale features
        print("Scaling features...")
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # Step 9: PCA
        print("Performing PCA...")
        pca = PCA(n_components=10)
        X_pca = pca.fit_transform(X_scaled)
        
        # Step 10: Combine data
        df_pca = pd.concat([
            df[['SMILES']],
            pd.DataFrame(X_pca, columns=[f'PCA{i+1}' for i in range(X_pca.shape[1])]),
            pd.Series(y, name='pIC50')
        ], axis=1)
        
        # Step 11: Split data
        print("Splitting data...")
        train_data, test_data = train_test_split(df_pca, test_size=0.2, random_state=42)
        train_data = train_data.reset_index(drop=True)
        test_data = test_data.reset_index(drop=True)
        
        # Store targets
        y_train = train_data['pIC50']
        y_test = test_data['pIC50']
        
        # Step 12: Save temporary files
        print("Saving temporary files...")
        temp_files = []
        with tempfile.NamedTemporaryFile(delete=False, mode='w+', suffix='.csv') as train_file, \
             tempfile.NamedTemporaryFile(delete=False, mode='w+', suffix='.csv') as test_file:
            
            train_data[['SMILES', 'pIC50']].to_csv(train_file.name, index=False)
            test_data[['SMILES', 'pIC50']].to_csv(test_file.name, index=False)
            train_file_path = train_file.name
            test_file_path = test_file.name
            temp_files.extend([train_file_path, test_file_path])
        
        # Step 13: Train model
        print("Training Chemprop model...")
        train_command = f"""
        chemprop_train \
            --data_path {train_file_path} \
            --separate_val_path {test_file_path} \
            --separate_test_path {test_file_path} \
            --save_dir model_output_wo \
            --epochs 2 \
            --batch_size 32 \
            --init_lr 0.0005 \
            --max_lr 0.001 \
            --final_lr 0.0001 \
            --hidden_size 1600 \
            --depth 5 \
            --dropout 0.35 \
            --ffn_hidden_size 300 \
            --ffn_num_layers 3 \
            --dataset_type regression \
            --gpu 0 \
            --no_features_scaling
        """
        os.system(train_command)
        
        # Step 14: Make predictions
        print("Making predictions...")
        test_preds_file = 'test_pred_wo.csv'
        train_preds_file = 'train_pred_wo.csv'
        
        # Predict on test set
        test_predict_command = f"""
        chemprop_predict \
            --test_path {test_file_path} \
            --checkpoint_dir model_output_wo/fold_0/model_0 \
            --preds_path {test_preds_file} \
            --no_features_scaling
        """
        os.system(test_predict_command)
        
        # Predict on train set
        train_predict_command = f"""
        chemprop_predict \
            --test_path {train_file_path} \
            --checkpoint_dir model_output_wo/fold_0/model_0 \
            --preds_path {train_preds_file} \
            --no_features_scaling
        """
        os.system(train_predict_command)
        
        # Step 15: Calculate metrics
        print("Calculating metrics...")
        try:
            # Load predictions
            test_preds_df = pd.read_csv(test_preds_file)
            train_preds_df = pd.read_csv(train_preds_file)
            
            # Extract and validate predictions
            test_preds = test_preds_df['predicted'].values if 'predicted' in test_preds_df else test_preds_df.iloc[:, -1].values
            train_preds = train_preds_df['predicted'].values if 'predicted' in train_preds_df else train_preds_df.iloc[:, -1].values
            
            # Ensure matching lengths
            train_preds = train_preds[:len(y_train)]
            
            # Handle NaN values
            test_preds = np.nan_to_num(test_preds, nan=0)
            train_preds = np.nan_to_num(train_preds, nan=0)
            y_test_np = y_test.fillna(0).values
            y_train_np = y_train.fillna(0).values
            
            # Calculate metrics
            metrics = {
                'test_metrics': {
                    'rmse': np.sqrt(mean_squared_error(y_test_np, test_preds)),
                    'mae': mean_absolute_error(y_test_np, test_preds),
                    'r2': r2_score(y_test_np, test_preds)
                },
                'train_metrics': {
                    'rmse': np.sqrt(mean_squared_error(y_train_np, train_preds)),
                    'mae': mean_absolute_error(y_train_np, train_preds),
                    'r2': r2_score(y_train_np, train_preds)
                }
            }
            
            # Print metrics
            for dataset in ['test_metrics', 'train_metrics']:
                print(f"\n{dataset.split('_')[0].title()} Set Metrics:")
                for metric, value in metrics[dataset].items():
                    print(f"{metric.upper()}: {value:.4f}")
                    
            return metrics
            
        except Exception as e:
            print(f"Error calculating metrics: {str(e)}")
            print("Debug information:")
            print("\nTest predictions file content (head):")
            try:
                print(pd.read_csv(test_preds_file).head())
            except Exception as e2:
                print(f"Error reading predictions: {str(e2)}")
            return None
            
    except Exception as e:
        print(f"An error occurred: {str(e)}")
        return None
        
    finally:
        # Cleanup
        print("Cleaning up temporary files...")
        for temp_file in temp_files:
            try:
                os.remove(temp_file)
            except Exception as e:
                print(f"Error removing temporary file {temp_file}: {str(e)}")

if __name__ == "__main__":
    print("Starting model training and evaluation...")
    metrics = preprocess_and_train()
    if metrics:
        print("\nProcess completed successfully.")
